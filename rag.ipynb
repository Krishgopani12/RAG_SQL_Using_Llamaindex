{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import required libraries\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.text_splitter import TokenTextSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole, ImageBlock\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine import SQLAutoVectorQueryEngine, RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexAutoRetriever\n",
    "from llama_index.core.vector_stores import MetadataInfo, VectorStoreInfo\n",
    "from llama_index.core import SQLDatabase\n",
    "import chromadb\n",
    "import sqlite3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Set up OpenAI API key\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the OpenAI API key from the environment variable\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Check if the API key is loaded correctly\n",
    "if openai.api_key:\n",
    "    print(\"API key loaded successfully!\")\n",
    "else:\n",
    "    print(\"API key not found. Please check your .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Function to get table information\n",
    "def get_table_info(db_path):\n",
    "    \"\"\"\n",
    "    Get information about tables in the SQLite database.\n",
    "    Returns a dictionary with table descriptions.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get all tables\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    \n",
    "    table_info = {}\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        # Get column information for each table\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "        columns = cursor.fetchall()\n",
    "        \n",
    "        # Create description of table structure\n",
    "        column_descriptions = [f\"{col[1]} ({col[2]})\" for col in columns]\n",
    "        table_info[table_name] = {\n",
    "            'columns': column_descriptions,\n",
    "            'description': f\"Table {table_name} contains columns: {', '.join(column_descriptions)}\"\n",
    "        }\n",
    "    \n",
    "    conn.close()\n",
    "    return table_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Initialize components\n",
    "# Define node parser and LLM\n",
    "chunk_size = 1024\n",
    "llm = OpenAI(temperature=0, model=\"gpt-4\", streaming=True)\n",
    "\n",
    "# Use Settings instead of ServiceContext\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = chunk_size\n",
    "\n",
    "# Initialize components\n",
    "text_splitter = TokenTextSplitter(chunk_size=chunk_size)\n",
    "node_parser = SimpleNodeParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Initialize Chroma\n",
    "# Initialize Chroma\n",
    "chroma_client = chromadb.PersistentClient(path=\"./vector_db\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"multi_modal_rag\")\n",
    "\n",
    "# Create vector store\n",
    "vector_store = ChromaVectorStore(\n",
    "    chroma_collection=chroma_collection\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "vector_index = VectorStoreIndex([], storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Set up vector store info and retriever\n",
    "# Set up vector store info\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"Database tables and their contents\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(name=\"table_name\", type=\"str\", description=\"The name of the database table\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "vector_auto_retriever = VectorIndexAutoRetriever(\n",
    "    vector_index, vector_store_info=vector_store_info\n",
    ")\n",
    "\n",
    "# Create retriever query engine\n",
    "retriever_query_engine = RetrieverQueryEngine.from_args(\n",
    "    vector_auto_retriever, llm=Settings.llm, chunk_size=Settings.chunk_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 7: Create SQL database connection and query engine\n",
    "# from llama_index.core import SQLDatabase\n",
    "# from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "\n",
    "# # Define the path to your SQLite database\n",
    "# sql_database_path = \"./database/olist.sqlite\"\n",
    "\n",
    "# # Create SQL database connection\n",
    "# sql_database = SQLDatabase.from_uri(\n",
    "#     f\"sqlite:///{sql_database_path}\"\n",
    "# )\n",
    "\n",
    "# # Get table information\n",
    "# table_info = get_table_info(sql_database_path)\n",
    "\n",
    "# # Create SQL query engine\n",
    "# sql_query_engine = NLSQLTableQueryEngine(\n",
    "#     sql_database=sql_database,\n",
    "#     tables=list(table_info.keys()),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Create SQL database connection and query engine\n",
    "from llama_index.core import SQLDatabase\n",
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "from llama_index.core.objects import SQLTableSchema\n",
    "\n",
    "# # Define the path to your SQLite database\n",
    "# sql_database_path = \"./database/olist.sqlite\"\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Get SQL database path from environment variable\n",
    "sql_database_path = os.getenv(\"SQL_DATABASE_PATH\", \"./database/olist.sqlite\")\n",
    "\n",
    "# Create SQL database connection\n",
    "sql_database = SQLDatabase.from_uri(\n",
    "    f\"sqlite:///{sql_database_path}\"\n",
    ")\n",
    "\n",
    "# Get table information\n",
    "table_info = get_table_info(sql_database_path)\n",
    "\n",
    "# Define table schemas with relationships\n",
    "table_schemas = {}\n",
    "for table_name in table_info.keys():\n",
    "    # Get foreign key information\n",
    "    conn = sqlite3.connect(sql_database_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"PRAGMA foreign_key_list({table_name});\")\n",
    "    foreign_keys = cursor.fetchall()\n",
    "    \n",
    "    # Create relationship context\n",
    "    relationships = []\n",
    "    for fk in foreign_keys:\n",
    "        from_col = fk[3]\n",
    "        to_table = fk[2]\n",
    "        to_col = fk[4]\n",
    "        relationships.append(f\"The {from_col} in {table_name} is related to {to_col} in {to_table}\")\n",
    "    \n",
    "    # Get column descriptions\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "    columns = cursor.fetchall()\n",
    "    column_descriptions = []\n",
    "    for col in columns:\n",
    "        col_name = col[1]\n",
    "        col_type = col[2]\n",
    "        column_descriptions.append(f\"{col_name} ({col_type})\")\n",
    "    \n",
    "    # Create table context\n",
    "    context = f\"Table {table_name} contains: {', '.join(column_descriptions)}\"\n",
    "    if relationships:\n",
    "        context += f\". Relationships: {'. '.join(relationships)}\"\n",
    "    \n",
    "    table_schemas[table_name] = SQLTableSchema(\n",
    "        table_name=table_name,\n",
    "        context_str=context\n",
    "    )\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "# Create SQL query engine with enhanced capabilities\n",
    "sql_query_engine = NLSQLTableQueryEngine(\n",
    "    sql_database=sql_database,\n",
    "    tables=list(table_info.keys()),\n",
    "    table_schemas=table_schemas,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create tools and final query engine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# Create tools\n",
    "sql_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=sql_query_engine,\n",
    "    name=\"sql_tool\",\n",
    "    description=(\n",
    "        \"Useful for answering questions that require querying structured data from the database. \"\n",
    "        \"This tool can analyze tables and their relationships to get specific data points. \"\n",
    "        f\"Available tables: {', '.join(table_info.keys())}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=retriever_query_engine,\n",
    "    name=\"vector_tool\",\n",
    "    description=(\n",
    "        \"Useful for answering semantic questions about the data that require understanding context \"\n",
    "        \"and relationships beyond simple queries. This tool can provide high-level insights and \"\n",
    "        \"analyze unstructured or complex information about the database contents.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create selector with LLM\n",
    "selector = LLMSingleSelector.from_defaults(llm=Settings.llm)\n",
    "\n",
    "# Create the final multi-tool query engine\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=selector,\n",
    "    query_engine_tools=[\n",
    "        sql_tool,\n",
    "        vector_tool\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.5: Create enhanced query engine with multi-tool chaining capability\n",
    "from typing import Optional, Dict, Any\n",
    "from llama_index.core.base.response.schema import Response\n",
    "\n",
    "class ChainedQueryEngine:\n",
    "    def __init__(self, sql_engine, vector_engine, llm):\n",
    "        self.sql_engine = sql_engine\n",
    "        self.vector_engine = vector_engine\n",
    "        self.llm = llm\n",
    "        \n",
    "    def _analyze_query_type(self, query: str) -> str:\n",
    "        \"\"\"Determine if the query needs SQL, vector search, or both.\"\"\"\n",
    "        prompt = f\"\"\"Analyze the following query and determine which type of processing it needs:\n",
    "        Query: {query}\n",
    "        \n",
    "        Choose one of:\n",
    "        1. \"sql\" - if it only needs structured database querying\n",
    "        2. \"vector\" - if it only needs semantic search in vector database\n",
    "        3. \"chained\" - if it needs both SQL first and then vector search\n",
    "        4. \"vector_then_sql\" - if it needs vector search first and then SQL\n",
    "        \n",
    "        Return ONLY the type (sql/vector/chained/vector_then_sql) without explanation.\"\"\"\n",
    "        \n",
    "        response = self.llm.complete(prompt)\n",
    "        return response.text.strip().lower()\n",
    "    \n",
    "    def _extract_entity_from_sql_result(self, sql_response: Response, query: str) -> str:\n",
    "        \"\"\"Extract relevant entity from SQL response to use in vector search.\"\"\"\n",
    "        prompt = f\"\"\"From the SQL query result and the original question, extract the key entity needed for the next search.\n",
    "        Original question: {query}\n",
    "        SQL result: {sql_response}\n",
    "        \n",
    "        Return ONLY the entity name without any explanation or additional text.\"\"\"\n",
    "        \n",
    "        response = self.llm.complete(prompt)\n",
    "        return response.text.strip()\n",
    "    \n",
    "    def _combine_responses(self, sql_response: Optional[Response], vector_response: Optional[Response], query: str) -> Response:\n",
    "        \"\"\"Combine responses from both engines into a coherent answer.\"\"\"\n",
    "        prompt = f\"\"\"Combine the following information into a coherent answer:\n",
    "        Original question: {query}\n",
    "        SQL database result: {sql_response if sql_response else 'Not used'}\n",
    "        Vector search result: {vector_response if vector_response else 'Not used'}\n",
    "        \n",
    "        Provide a natural, flowing response that answers the original question.\"\"\"\n",
    "        \n",
    "        combined_response = self.llm.complete(prompt)\n",
    "        \n",
    "        # Create a new Response object with combined information\n",
    "        response = Response(response=combined_response.text)\n",
    "        if sql_response and hasattr(sql_response, 'metadata'):\n",
    "            response.metadata = {'sql_metadata': sql_response.metadata}\n",
    "        if vector_response and hasattr(vector_response, 'metadata'):\n",
    "            response.metadata = {**(response.metadata or {}), 'vector_metadata': vector_response.metadata}\n",
    "        \n",
    "        # Combine source nodes if available\n",
    "        source_nodes = []\n",
    "        if sql_response and hasattr(sql_response, 'source_nodes'):\n",
    "            source_nodes.extend(sql_response.source_nodes)\n",
    "        if vector_response and hasattr(vector_response, 'source_nodes'):\n",
    "            source_nodes.extend(vector_response.source_nodes)\n",
    "        if source_nodes:\n",
    "            response.source_nodes = source_nodes\n",
    "            \n",
    "        return response\n",
    "    \n",
    "    def _format_vector_query(self, original_query: str, sql_result: str) -> str:\n",
    "        \"\"\"Format a new query for vector search based on SQL results.\"\"\"\n",
    "        prompt = f\"\"\"Given the original question and SQL result, create a focused query for semantic search:\n",
    "        Original question: {original_query}\n",
    "        SQL result: {sql_result}\n",
    "        \n",
    "        Return ONLY the reformulated query without any explanation.\"\"\"\n",
    "        \n",
    "        response = self.llm.complete(prompt)\n",
    "        return response.text.strip()\n",
    "    \n",
    "    def query(self, query_str: str) -> Response:\n",
    "        \"\"\"Process a query using appropriate combination of SQL and vector search.\"\"\"\n",
    "        query_type = self._analyze_query_type(query_str)\n",
    "        \n",
    "        if query_type == \"sql\":\n",
    "            return self.sql_engine.query(query_str)\n",
    "        \n",
    "        elif query_type == \"vector\":\n",
    "            return self.vector_engine.query(query_str)\n",
    "        \n",
    "        elif query_type == \"chained\":\n",
    "            # First get SQL result\n",
    "            sql_response = self.sql_engine.query(query_str)\n",
    "            \n",
    "            # Extract relevant entity from SQL result\n",
    "            entity = self._extract_entity_from_sql_result(sql_response, query_str)\n",
    "            \n",
    "            # Format new query for vector search\n",
    "            vector_query = self._format_vector_query(query_str, str(sql_response))\n",
    "            \n",
    "            # Perform vector search\n",
    "            vector_response = self.vector_engine.query(vector_query)\n",
    "            \n",
    "            # Combine responses\n",
    "            return self._combine_responses(sql_response, vector_response, query_str)\n",
    "        \n",
    "        elif query_type == \"vector_then_sql\":\n",
    "            # First get vector search result\n",
    "            vector_response = self.vector_engine.query(query_str)\n",
    "            \n",
    "            # Format new query for SQL\n",
    "            sql_query = self._format_vector_query(query_str, str(vector_response))\n",
    "            \n",
    "            # Perform SQL query\n",
    "            sql_response = self.sql_engine.query(sql_query)\n",
    "            \n",
    "            # Combine responses\n",
    "            return self._combine_responses(sql_response, vector_response, query_str)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown query type: {query_type}\")\n",
    "\n",
    "# Create the enhanced query engine\n",
    "enhanced_query_engine = ChainedQueryEngine(\n",
    "    sql_engine=sql_query_engine,\n",
    "    vector_engine=retriever_query_engine,\n",
    "    llm=Settings.llm\n",
    ")\n",
    "\n",
    "# Update the query engine to use the enhanced version\n",
    "query_engine = enhanced_query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9: Example usage function\n",
    "# def ask_question(question: str):\n",
    "#     \"\"\"\n",
    "#     Ask a question about the database using both SQL and vector search capabilities.\n",
    "    \n",
    "#     Args:\n",
    "#         question (str): The question to ask about the database\n",
    "        \n",
    "#     Returns:\n",
    "#         str: The response from the query engine\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         response = query_engine.query(question)\n",
    "#         return response\n",
    "#     except Exception as e:\n",
    "#         return f\"Error processing question: {str(e)}\"\n",
    "\n",
    "# # Example usage\n",
    "# # You can uncomment and run these examples:\n",
    "# \"\"\"\n",
    "# questions = [\n",
    "#     \"How many orders are there in total?\",\n",
    "#     \"What is the distribution of order status?\",\n",
    "#     \"What are the most common payment types?\",\n",
    "#     \"Which sellers have the highest customer ratings?\",\n",
    "# ]\n",
    "\n",
    "# for question in questions:\n",
    "#     print(f\"\\nQuestion: {question}\")\n",
    "#     print(f\"Answer: {ask_question(question)}\")\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Enhanced question answering with debug information\n",
    "from typing import Any, List\n",
    "from llama_index.core.base.response.schema import Response\n",
    "\n",
    "def debug_query_engine_response(response: Response) -> None:\n",
    "    \"\"\"Print debug information from a query engine response.\"\"\"\n",
    "    print(\"\\n=== Debug Information ===\")\n",
    "    \n",
    "    # Get source nodes\n",
    "    source_nodes = response.source_nodes\n",
    "    if source_nodes:\n",
    "        print(\"\\nSource Nodes Used:\")\n",
    "        for idx, node in enumerate(source_nodes):\n",
    "            print(f\"\\nSource {idx + 1}:\")\n",
    "            print(f\"Text: {node.text[:200]}...\")  # Print first 200 chars\n",
    "            if node.metadata:\n",
    "                print(f\"Metadata: {node.metadata}\")\n",
    "            print(f\"Score: {node.score if hasattr(node, 'score') else 'N/A'}\")\n",
    "    \n",
    "    # Get tool used (if available)\n",
    "    if hasattr(response, '_tool_used'):\n",
    "        print(f\"\\nTool Used: {response._tool_used}\")\n",
    "    \n",
    "    # Print any SQL queries that were executed\n",
    "    if hasattr(response, 'metadata') and response.metadata:\n",
    "        if 'sql_query' in response.metadata:\n",
    "            print(f\"\\nSQL Query Executed:\")\n",
    "            print(response.metadata['sql_query'])\n",
    "        if 'sql_response' in response.metadata:\n",
    "            print(f\"\\nSQL Response:\")\n",
    "            print(response.metadata['sql_response'])\n",
    "\n",
    "def ask_question(question: str, debug: bool = True):\n",
    "    \"\"\"\n",
    "    Ask a question about the database using both SQL and vector search capabilities.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to ask about the database\n",
    "        debug (bool): Whether to show debug information\n",
    "        \n",
    "    Returns:\n",
    "        str: The response from the query engine\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\nü§î Question: {question}\")\n",
    "        \n",
    "        # Get the response from the query engine\n",
    "        response = query_engine.query(question)\n",
    "        \n",
    "        print(f\"\\nüìù Answer: {response}\")\n",
    "        \n",
    "        if debug:\n",
    "            debug_query_engine_response(response)\n",
    "            \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error processing question: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = ask_question(\"Explain multi head attention?\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = ask_question(\"what are the top 5 selling products by price?\")\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = ask_question(\"\"\"\n",
    "#     Who are the top 5 sellers based on a combination of:\n",
    "#     - Total order volume\n",
    "#     - Average customer rating\n",
    "#     - Average delivery time\n",
    "#     Provide their performance metrics.\n",
    "# \"\"\")\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = ask_question(\"What is the population of Tokyo?\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§î Question: history of the city tokyo?\n",
      "\n",
      "üìù Answer: Tokyo started as a small fishing village named Edo in the old Musashi Province. The Edo clan built walls to protect the town in the late 12th century, and in 1457, ≈åta D≈çkan built Edo Castle. In 1590, Tokugawa Ieyasu made Edo his base and became shogun in 1603, making the town the center of his military government. This marked the beginning of the Edo period, during which Edo grew into one of the largest cities in the world. \n",
      "\n",
      "Although Edo was not the capital of Japan, it held more power due to the shogun's influence. The Meiji Restoration in 1869 removed the shogun from power, and the 17-year-old Emperor Meiji moved to Edo, transforming the old Edo Castle into the Kokyo, the Imperial Palace. \n",
      "\n",
      "In the same year, the Tokyo Prefecture and the city of Tokyo were established, serving as the capital city until 1943. In 1943, Tokyo City and the associated municipalities of what was formerly Tokyo Prefecture combined into one. \n",
      "\n",
      "Tokyo suffered two major catastrophes in the 20th century: the 1923 Great Kant≈ç earthquake and the Bombing of Tokyo during World War II. The city was completely rebuilt after the war, with new high-rise developments starting in the 1970s.\n",
      "\n",
      "=== Debug Information ===\n",
      "\n",
      "Source Nodes Used:\n",
      "\n",
      "Source 1:\n",
      "Text: History\n",
      "\n",
      "1457-1869\n",
      "\n",
      "Tokyo began as a small fishing village named Edo(Ê±üÊà∏„ÄÅ„Åà„Å©).[7] Edo was in the old Musashi Province.[8]\n",
      "\n",
      "The Edo clan built walls to protect the town in the late 12th century. In 1457,...\n",
      "Metadata: {'doc_id': '2a49fe81-990b-4990-89fa-7d9d91eb12b8'}\n",
      "Score: 0.7944129286020037\n",
      "\n",
      "Source 2:\n",
      "Text: Shinagawa\n",
      "\n",
      "Edogawa Nerima Tait≈ç\n",
      "\n",
      "Itabashi ≈åta Toshima\n",
      "\n",
      "Katsushika Setagaya\n",
      "\n",
      "Three wards of Tokyo make up the central part of the city. They are Chiyoda, Ch≈´≈ç and Minato.[20]\n",
      "\n",
      "Landmarks\n",
      "\n",
      "Tokyo has many...\n",
      "Metadata: {'doc_id': '5462fdd8-c788-49ee-bf89-374b42ed5a5f'}\n",
      "Score: 0.7250832088315674\n",
      "Tokyo started as a small fishing village named Edo in the old Musashi Province. The Edo clan built walls to protect the town in the late 12th century, and in 1457, ≈åta D≈çkan built Edo Castle. In 1590, Tokugawa Ieyasu made Edo his base and became shogun in 1603, making the town the center of his military government. This marked the beginning of the Edo period, during which Edo grew into one of the largest cities in the world. \n",
      "\n",
      "Although Edo was not the capital of Japan, it held more power due to the shogun's influence. The Meiji Restoration in 1869 removed the shogun from power, and the 17-year-old Emperor Meiji moved to Edo, transforming the old Edo Castle into the Kokyo, the Imperial Palace. \n",
      "\n",
      "In the same year, the Tokyo Prefecture and the city of Tokyo were established, serving as the capital city until 1943. In 1943, Tokyo City and the associated municipalities of what was formerly Tokyo Prefecture combined into one. \n",
      "\n",
      "Tokyo suffered two major catastrophes in the 20th century: the 1923 Great Kant≈ç earthquake and the Bombing of Tokyo during World War II. The city was completely rebuilt after the war, with new high-rise developments starting in the 1970s.\n"
     ]
    }
   ],
   "source": [
    "response = ask_question(\"history of the city tokyo?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tokyo started as a small fishing village named Edo in the old Musashi Province. The Edo clan built walls to protect the town in the late 12th century, and in 1457, ≈åta D≈çkan built Edo Castle. In 1590, Tokugawa Ieyasu made Edo his base and became shogun in 1603, making the town the center of his military government. This marked the beginning of the Edo period, during which Edo grew into one of the largest cities in the world. \\n\\nAlthough Edo was not the capital of Japan, it held more power due to the shogun's influence. The Meiji Restoration in 1869 removed the shogun from power, and the 17-year-old Emperor Meiji moved to Edo, transforming the old Edo Castle into the Kokyo, the Imperial Palace. \\n\\nIn the same year, the Tokyo Prefecture and the city of Tokyo were established, serving as the capital city until 1943. In 1943, Tokyo City and the associated municipalities of what was formerly Tokyo Prefecture combined into one. \\n\\nTokyo suffered two major catastrophes in the 20th century: the 1923 Great Kant≈ç earthquake and the Bombing of Tokyo during World War II. The city was completely rebuilt after the war, with new high-rise developments starting in the 1970s.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sql_metadata': {'45daa04a-2fe6-4c22-9104-766a9ce77843': {'sql_query': 'SELECT product_id, price \\nFROM order_items \\nORDER BY price DESC \\nLIMIT 5;',\n",
       "   'result': [('489ae2aa008f021502940f251d4cce7f', 6735.0),\n",
       "    ('69c590f7ffc7bf8db97190b6cb6ed62e', 6729.0),\n",
       "    ('1bdf5e6731585cf01aa8169c7028d6ad', 6499.0),\n",
       "    ('a6492cc69376c469ab6f61d8f44de961', 4799.0),\n",
       "    ('c3ed642d592594bb648ff4a04cee2747', 4690.0)],\n",
       "   'col_keys': ['product_id', 'price']},\n",
       "  'sql_query': 'SELECT product_id, price \\nFROM order_items \\nORDER BY price DESC \\nLIMIT 5;',\n",
       "  'result': [('489ae2aa008f021502940f251d4cce7f', 6735.0),\n",
       "   ('69c590f7ffc7bf8db97190b6cb6ed62e', 6729.0),\n",
       "   ('1bdf5e6731585cf01aa8169c7028d6ad', 6499.0),\n",
       "   ('a6492cc69376c469ab6f61d8f44de961', 4799.0),\n",
       "   ('c3ed642d592594bb648ff4a04cee2747', 4690.0)],\n",
       "  'col_keys': ['product_id', 'price']},\n",
       " 'vector_metadata': {'95e0ba06-eb60-4b1d-8fe3-2c5a5b01cbb2': {'doc_id': '4293812d-1b80-46b4-ac4a-e2e1d4e128e8'},\n",
       "  'd72c8178-4ae3-4aa9-9832-94906005dc0f': {'doc_id': '23479f72-00d0-4785-b01a-aa179137a3a9'}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
